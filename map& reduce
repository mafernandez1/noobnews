package spark;

import com.google.common.collect.Iterables;
import com.google.common.collect.Sets;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import scala.Tuple2;
import utils.ISO8601;

import java.io.IOException;
import java.text.ParseException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Set;

public class PageRank {

    private static String usage() {
        StringBuilder sb = new StringBuilder();
        sb.append("USAGE:").append(System.lineSeparator());
        sb.append("\t- input path in HDFS").append(System.lineSeparator());
        sb.append("\t- output path in HDFS").append(System.lineSeparator());
        sb.append("\t- number of page rank iterations (default = 1)").append(System.lineSeparator());
        sb.append("\t- ISO8601 format of date").append(System.lineSeparator());
        return sb.toString();
    }

    private static long retrieveTimestamp(String line) {
        try {
            return ISO8601.toTimeMS(line);
        } catch (ParseException e) {
            return 0L;
        }
    }

    private static boolean hasCompleteRevisionInfo(String[] revision) {
        return revision.length == 6;
    }

    private static JavaPairRDD<String, Iterable<String>> preprocess(
            JavaPairRDD<LongWritable, Text> input,
            long dateCondition
    ) {
        return input.map(t -> t._2.toString())
                .filter(t -> {
                    String[] revisionLine = t.split(System.lineSeparator())[0].trim().split("\\s");
                    return !t.isEmpty()
                            && hasCompleteRevisionInfo(revisionLine)
                            && retrieveTimestamp(revisionLine[3]) <= dateCondition;
                })
                /*
                 * emits (article_title revision_id<<>>MAIN link_1, link_2 .. link_n
                 */
                .mapToPair(new PairFunction<String, String, String>() {
                    @Override
                    public Tuple2<String, String> call(String s) throws Exception {
                        String[] lines = s.split("\n");
                        String[] revisionLines = lines[0].trim().split(" ");
                        String outlinks = lines[3];
                        return new Tuple2<>(revisionLines[2], revisionLines[1] + "<<>>" + outlinks);
                    }
                })
                .reduceByKey(new Function2<String, String, String>() {
                    @Override
                    public String call(String v1, String v2) throws Exception {
                        String[] a = v1.split("<<>>");
                        String[] b = v2.split("<<>>");

                        int revIdA = Integer.parseInt(a[0]);
                        int revIdB = Integer.parseInt(b[0]);

                        return revIdA > revIdB ? v1 : v2;
                    }
                })
                .flatMapValues(new Function<String, Iterable<String>>() {
                    @Override
                    public Iterable<String> call(String v1) throws Exception {
                        if (v1.endsWith("<<>>MAIN")) {
                            v1 = "<<NOLINKS>>";
                        }
                        return Arrays.asList(v1.replaceAll("\\s", " ").split(" "));
                    }
                })
                .filter(v1 -> !v1._2.endsWith("<<>>MAIN"))
                .groupByKey()
                .mapToPair(new PairFunction<Tuple2<String, Iterable<String>>, String, Iterable<String>>() {
                    @Override
                    public Tuple2<String, Iterable<String>> call(Tuple2<String, Iterable<String>> v) throws Exception {
                        Set<String> outlinks = Sets.newHashSet(v._2);
                        outlinks.remove(v._1);

                        return new Tuple2<>(v._1, outlinks);
                    }
                });
    }

    private static JavaPairRDD<String, Double> pagerank(
            JavaPairRDD<String, Iterable<String>> links,
            JavaPairRDD<String, Double> existingScores
    ) {
        JavaPairRDD<String, Double> contrib = links
                .join(existingScores)
                .flatMapToPair(v -> {
                    String articleKey = v._1;
                    Double articleScore = v._2._2;
                    Iterable<String> outlinks = v._2._1;
                    int numberOfOutlinks = Iterables.size(v._2._1);

                    List<Tuple2<String, Double>> results = new ArrayList<>();
                    results.add(new Tuple2<>(articleKey, 0.0));

                    for (String s : outlinks) {
                        if (s.equals("<<NOLINKS>>")) {
                            numberOfOutlinks -= 1;
                        }
                    }

                    for (String s : outlinks) {
                        if (s.equals("<<NOLINKS>>")) {
                            continue;
                        }

                        results.add(new Tuple2<>(s, articleScore / numberOfOutlinks));
                    }

                    return results;
                });

        return contrib.reduceByKey((acc, x) -> acc+x).mapValues(sum -> 0.15 + 0.85 * sum);
    }

    private static JavaPairRDD<String, Double> sortScores(
            JavaPairRDD<String, Double> scores
    ) {
        return scores.mapToPair(Tuple2::swap)
        .sortByKey(false)
        .mapToPair(Tuple2::swap);
    }

    public static void main(String[] args) throws InterruptedException, IOException {
        if (args.length < 3) {
            System.out.println(usage());
            System.exit(1);
        }

        final String inputFilePath = args[0];
        final String outputPath = args[1];
        final int iterations = Integer.parseInt(args[2]);
        final long date = retrieveTimestamp(args[3]);

        final boolean runLocally = System.getenv("RUN_LOCAL") != null;

        SparkConf sparkConf = new SparkConf();
        sparkConf.setAppName("PageRank")
                .set("spark.executor.instances", "7")
                .set("spark.executor.cores", "4")
                .set("spark.executor.memory", "3584m");

        if (runLocally) {
            sparkConf.setMaster("local");
        }

        try (JavaSparkContext sc = new JavaSparkContext(sparkConf)) {
            Configuration conf = new Configuration();
            conf.set("textinputformat.record.delimiter", "REVISION");
            JavaPairRDD<LongWritable, Text> ds = sc.newAPIHadoopFile(inputFilePath,
                    TextInputFormat.class,
                    LongWritable.class,
                    Text.class, conf);

            JavaPairRDD<String, Iterable<String>> articleAndLinks = preprocess(ds, date).cache();
            JavaPairRDD<String, Double> ranks = articleAndLinks.mapValues(v -> 1.0);

            for (int i = 0; i < iterations; i++) {
                ranks = pagerank(articleAndLinks, ranks);
                if (runLocally) {
                    ranks.repartition(1)
                            .saveAsTextFile(String.format("%s/preprocess-%d", outputPath, i));
                }
            }

            ranks = sortScores(ranks);

            ranks.map(x -> x._1 + " " + x._2)
            .coalesce(1)
            .saveAsTextFile(String.format("%s/final", outputPath));
        }
    }
}
